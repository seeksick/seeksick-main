#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ë©”ì¸ í”„ë¡œê·¸ë¨

ì´ í”„ë¡œê·¸ë¨ì€ ë‹¤ìŒ 3ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤:
1. ì–¼êµ´ ê°ì • (seeksick-resnet18.pth)
2. ìŒì„± ê°ì • (seeksick-voice.pt)
3. í…ìŠ¤íŠ¸ ê°ì • (seeksick-kobert.pt)

ëª¨ë“  ê°ì •ì€ [í–‰ë³µ, ìš°ìš¸, ë†€ëŒ, í™”ë‚¨, ì¤‘ë¦½] 5ê°€ì§€ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.
"""

import os
import sys
import time
import threading
import queue
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import numpy as np
import cv2
import torch
import torch.nn.functional as F
import sounddevice as sd
from scipy.io import wavfile
import whisper

# ë¡œì»¬ ëª¨ë¸ ì„í¬íŠ¸
from models import FaceEmotionAnalyzer, VoiceEmotionAnalyzer, TextEmotionAnalyzer, EMOTIONS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì „ì—­ ìƒìˆ˜
SAMPLE_RATE = 16000  # Whisper ê¶Œì¥ ìƒ˜í”Œë ˆì´íŠ¸
AUDIO_BUFFER_SIZE = 3.0  # 3ì´ˆ ë‹¨ìœ„ë¡œ ìŒì„± ë¶„ì„
VIDEO_FPS = 10  # ì–¼êµ´ ê°ì • ë¶„ì„ ì£¼ê¸°
FUSION_INTERVAL = 5.0  # Late Fusion ê°„ê²© (5ì´ˆ)

# ANSI ìƒ‰ìƒ ì½”ë“œ
class Colors:
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

class LateFusion:
    """Late Fusion - ë©€í‹°ëª¨ë‹¬ ê°ì • í†µí•© í´ë˜ìŠ¤ (ê°€ì¤‘ í‰ê· )"""
    
    def __init__(self, interval: float = FUSION_INTERVAL):
        self.interval = interval
        self.face_buffer = []
        self.voice_buffer = []
        self.text_buffer = []
        self.last_fusion_time = time.time()
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹, Î± = 0.6)
        # ì •í™•ë„: ì–¼êµ´ 74%, ìŒì„± 65%, í…ìŠ¤íŠ¸(LLM) 66%
        accuracies = {'face': 74.0, 'voice': 65.0, 'text': 66.0}
        total_acc = sum(accuracies.values())
        
        # ì„±ëŠ¥ ë¹„ë¡€ ê°€ì¤‘ì¹˜
        perf_weights = {k: v/total_acc for k, v in accuracies.items()}
        # face: 0.361, voice: 0.317, text: 0.322
        
        # ê· ë“± ê°€ì¤‘ì¹˜
        equal_weight = 1.0 / 3.0
        
        # í•˜ì´ë¸Œë¦¬ë“œ (Î± = 0.6: 60% ì„±ëŠ¥ ê¸°ë°˜, 40% ê· ë“±)
        alpha = 0.6
        self.weights = {
            'face': alpha * perf_weights['face'] + (1-alpha) * equal_weight,
            'voice': alpha * perf_weights['voice'] + (1-alpha) * equal_weight,
            'text': alpha * perf_weights['text'] + (1-alpha) * equal_weight
        }
        # ê²°ê³¼: face â‰ˆ 0.350, voice â‰ˆ 0.323, text â‰ˆ 0.326
        
    def add_face_emotion(self, probs: np.ndarray):
        """ì–¼êµ´ ê°ì • ê²°ê³¼ ì¶”ê°€"""
        self.face_buffer.append(probs)
    
    def add_voice_emotion(self, probs: np.ndarray):
        """ìŒì„± ê°ì • ê²°ê³¼ ì¶”ê°€"""
        if probs is not None:
            self.voice_buffer.append(probs)
    
    def add_text_emotion(self, probs: np.ndarray):
        """í…ìŠ¤íŠ¸ ê°ì • ê²°ê³¼ ì¶”ê°€"""
        if probs is not None:
            self.text_buffer.append(probs)
    
    def should_fuse(self) -> bool:
        """ìœµí•©í•  ì‹œê°„ì¸ì§€ í™•ì¸"""
        return (time.time() - self.last_fusion_time) >= self.interval
    
    def fuse_emotions(self) -> Optional[Tuple[str, np.ndarray, Dict[str, int]]]:
        """
        ê°€ì¤‘ í‰ê·  Late Fusion: 5ì´ˆê°„ ìŒ“ì¸ ëª¨ë“  ê²°ê³¼ë¥¼ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í†µí•©
        
        ê°€ì¤‘ì¹˜:
        - ì–¼êµ´ (74% ì •í™•ë„): 0.350
        - ìŒì„± (65% ì •í™•ë„): 0.323
        - í…ìŠ¤íŠ¸ (66% ì •í™•ë„): 0.326
        
        Returns:
            (ìµœì¢…_ê°ì •, 5ê°€ì§€_í™•ë¥ _ë°°ì—´, ëª¨ë‹¬ë¦¬í‹°_ì •ë³´) ë˜ëŠ” None
        """
        available_modalities = {}
        fused_probs = np.zeros(len(EMOTIONS))
        total_weight = 0.0
        
        # 1. ì–¼êµ´ ê°ì • í‰ê·  (ê°€ì¤‘ì¹˜ ì ìš©)
        if self.face_buffer:
            face_avg = np.mean(self.face_buffer, axis=0)
            fused_probs += face_avg * self.weights['face']
            total_weight += self.weights['face']
            available_modalities['face'] = len(self.face_buffer)
        
        # 2. ìŒì„± ê°ì • í‰ê·  (ê°€ì¤‘ì¹˜ ì ìš©)
        if self.voice_buffer:
            voice_avg = np.mean(self.voice_buffer, axis=0)
            fused_probs += voice_avg * self.weights['voice']
            total_weight += self.weights['voice']
            available_modalities['voice'] = len(self.voice_buffer)
        
        # 3. í…ìŠ¤íŠ¸ ê°ì • í‰ê·  (ê°€ì¤‘ì¹˜ ì ìš©)
        if self.text_buffer:
            text_avg = np.mean(self.text_buffer, axis=0)
            fused_probs += text_avg * self.weights['text']
            total_weight += self.weights['text']
            available_modalities['text'] = len(self.text_buffer)
        
        # ê²°ê³¼ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ None ë°˜í™˜
        if total_weight == 0:
            return None
        
        # ê°€ì¤‘ í‰ê·  ì •ê·œí™” (í™•ë¥  í•© = 1.0 ë³´ì¥)
        fused_probs /= total_weight
        
        # ìµœì¢… ê°ì • ì¶”ì¶œ (í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê²ƒ)
        max_idx = np.argmax(fused_probs)
        final_emotion = EMOTIONS[max_idx]
        
        return final_emotion, fused_probs, available_modalities
    
    def reset_buffers(self):
        """ë²„í¼ ì´ˆê¸°í™”"""
        self.face_buffer = []
        self.voice_buffer = []
        self.text_buffer = []
        self.last_fusion_time = time.time()
    
    def print_fusion_result(self, emotion: str, all_probs: np.ndarray, modalities: Dict[str, int]):
        """ìœµí•© ê²°ê³¼ë¥¼ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ì¶œë ¥ (5ê°€ì§€ ê°ì • í™•ë¥  ëª¨ë‘ í‘œì‹œ)"""
        print("\n" + "="*80)
        print(f"{Colors.BOLD}{Colors.RED}ğŸ¯ ê°€ì¤‘ í‰ê·  LATE FUSION ê²°ê³¼ (5ì´ˆ í†µí•©){Colors.END}")
        print("="*80)
        
        # ê°€ì¤‘ì¹˜ ì •ë³´
        print(f"\nâš–ï¸  ê°€ì¤‘ì¹˜ ì„¤ì •:")
        print(f"   ğŸ‘¤ ì–¼êµ´: {self.weights['face']:.3f} (ì •í™•ë„ 74%)")
        print(f"   ğŸ¤ ìŒì„±: {self.weights['voice']:.3f} (ì •í™•ë„ 65%)")
        print(f"   ğŸ“ í…ìŠ¤íŠ¸: {self.weights['text']:.3f} (ì •í™•ë„ 66%)")
        
        # ì‚¬ìš©ëœ ëª¨ë‹¬ë¦¬í‹° ì •ë³´
        print(f"\nğŸ“Š ì‚¬ìš©ëœ ëª¨ë‹¬ë¦¬í‹°:")
        if 'face' in modalities:
            print(f"   ğŸ‘¤ ì–¼êµ´: {modalities['face']}ê°œ ê²°ê³¼")
        if 'voice' in modalities:
            print(f"   ğŸ¤ ìŒì„±: {modalities['voice']}ê°œ ê²°ê³¼")
        if 'text' in modalities:
            print(f"   ğŸ“ í…ìŠ¤íŠ¸: {modalities['text']}ê°œ ê²°ê³¼")
        
        total_results = sum(modalities.values())
        print(f"\n   ì´ {len(modalities)}ê°œ ëª¨ë‹¬ë¦¬í‹°, {total_results}ê°œ ê²°ê³¼ í†µí•©")
        
        # 5ê°€ì§€ ê°ì • í™•ë¥  (ë¹¨ê°„ìƒ‰, êµµê²Œ)
        print(f"\n{Colors.BOLD}{Colors.RED}{'='*80}{Colors.END}")
        print(f"{Colors.BOLD}{Colors.RED}ğŸ“ˆ 5ê°€ì§€ ê°ì • í™•ë¥  ë¶„í¬:{Colors.END}")
        print(f"{Colors.BOLD}{Colors.RED}{'='*80}{Colors.END}")
        
        # ì´ëª¨ì§€ ë§¤í•‘
        emotion_emojis = {
            'happy': 'ğŸ˜Š',
            'depressed': 'ğŸ˜¢',
            'surprised': 'ğŸ˜®',
            'angry': 'ğŸ˜ ',
            'neutral': 'ğŸ˜'
        }
        
        # í™•ë¥  ìˆœì„œëŒ€ë¡œ ì •ë ¬
        sorted_indices = np.argsort(all_probs)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ
        
        for idx in sorted_indices:
            emo = EMOTIONS[idx]
            prob = float(all_probs[idx])
            emoji = emotion_emojis.get(emo, 'â“')
            
            # ë°” ê·¸ë˜í”„ ìƒì„± (40ì¹¸ ê¸°ì¤€)
            bar_length = int(prob * 40)
            bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
            
            # ìµœê³  í™•ë¥ ì´ë©´ ë¹¨ê°„ìƒ‰ + êµµê²Œ
            if idx == sorted_indices[0]:
                print(f"{Colors.BOLD}{Colors.RED}   {emoji} {emo:12s} [{bar}] {prob:.1%} â­{Colors.END}")
            else:
                print(f"   {emoji} {emo:12s} [{bar}] {prob:.1%}")
        
        # ìµœì¢… ê°ì • ê°•ì¡°
        max_prob = float(all_probs[sorted_indices[0]])
        print(f"\n{Colors.BOLD}{Colors.RED}{'='*80}{Colors.END}")
        print(f"{Colors.BOLD}{Colors.RED}   ğŸ† ìµœì¢… ê°ì •: {emotion.upper()} (ì‹ ë¢°ë„: {max_prob:.1%}){Colors.END}")
        print(f"{Colors.BOLD}{Colors.RED}{'='*80}{Colors.END}")
        
        print()

class AudioRecorder:
    """ìŒì„± ë…¹ìŒ ë° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, sample_rate: int = SAMPLE_RATE, buffer_duration: float = AUDIO_BUFFER_SIZE):
        self.sample_rate = sample_rate
        self.buffer_duration = buffer_duration
        self.buffer_size = int(sample_rate * buffer_duration)
        self.audio_queue = queue.Queue()
        self.is_recording = False
        self.audio_data = []
        
    def audio_callback(self, indata, frames, time, status):
        """ìŒì„± ë°ì´í„° ì½œë°± í•¨ìˆ˜"""
        if status:
            logger.warning(f"Audio callback status: {status}")
        self.audio_data.extend(indata[:, 0])  # ëª¨ë…¸ ì±„ë„ë§Œ ì‚¬ìš©
        
        # ë²„í¼ê°€ ê°€ë“ ì°¼ì„ ë•Œ íì— ì¶”ê°€
        if len(self.audio_data) >= self.buffer_size:
            audio_chunk = np.array(self.audio_data[:self.buffer_size], dtype=np.float32)
            self.audio_queue.put(audio_chunk)
            self.audio_data = self.audio_data[self.buffer_size:]
    
    def start_recording(self):
        """ìŒì„± ë…¹ìŒ ì‹œì‘"""
        self.is_recording = True
        self.stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            callback=self.audio_callback,
            blocksize=1024
        )
        self.stream.start()
        logger.info("ìŒì„± ë…¹ìŒì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def stop_recording(self):
        """ìŒì„± ë…¹ìŒ ì¤‘ì§€"""
        self.is_recording = False
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
        logger.info("ìŒì„± ë…¹ìŒì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_audio_chunk(self) -> Optional[np.ndarray]:
        """ìŒì„± ì²­í¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.audio_queue.get_nowait()
        except queue.Empty:
            return None

# ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ì€ models íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©

class MultimodalEmotionAnalyzer:
    """ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.audio_recorder = AudioRecorder()
        self.face_analyzer = FaceEmotionAnalyzer()
        self.voice_analyzer = VoiceEmotionAnalyzer()
        self.text_analyzer = TextEmotionAnalyzer()
        
        # Late Fusion ì´ˆê¸°í™”
        self.late_fusion = LateFusion(interval=FUSION_INTERVAL)
        
        # Whisper ëª¨ë¸ ë¡œë“œ
        self.whisper_model = whisper.load_model("base")
        logger.info("Whisper ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # ë¹„ë””ì˜¤ ìº¡ì²˜
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            logger.error("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        # ê²°ê³¼ ì €ì¥
        self.results = {
            'face': [],
            'voice': [],
            'text': [],
            'timestamps': []
        }
        
        self.is_running = False
        
    def transcribe_audio(self, audio_data: np.ndarray) -> str:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            # WhisperëŠ” float32 ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
            result = self.whisper_model.transcribe(audio_data)
            text = result["text"].strip()
            logger.info(f"ìŒì„± ì¸ì‹ ê²°ê³¼: {text}")
            return text
        except Exception as e:
            logger.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
            return ""
    
    def process_audio_thread(self):
        """ìŒì„± ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        while self.is_running:
            audio_chunk = self.audio_recorder.get_audio_chunk()
            if audio_chunk is not None:
                timestamp = datetime.now()
                
                # 1. ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = self.transcribe_audio(audio_chunk)
                
                # 2. ìŒì„± ê°ì • ë¶„ì„
                voice_probs = self.voice_analyzer.analyze_voice(audio_chunk)
                
                # 3. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
                text_probs = self.text_analyzer.analyze_text(text) if text else None
                
                # Late Fusionì— ê²°ê³¼ ì¶”ê°€
                if voice_probs is not None:
                    self.late_fusion.add_voice_emotion(voice_probs)
                if text_probs is not None:
                    self.late_fusion.add_text_emotion(text_probs)
                
                # ê²°ê³¼ ì €ì¥
                self.results['voice'].append(voice_probs)
                self.results['text'].append(text_probs)
                self.results['timestamps'].append(timestamp)
                
                # ê²°ê³¼ ì¶œë ¥
                self.print_emotion_results(timestamp, voice_probs, text_probs, text)
            
            time.sleep(0.1)
    
    def print_emotion_results(self, timestamp, voice_probs, text_probs, text=""):
        """ê°ì • ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ì‹œê°„: {timestamp.strftime('%H:%M:%S')}")
        print(f"í…ìŠ¤íŠ¸: {text}")
        print(f"{'='*60}")
        
        if voice_probs is not None:
            print("ğŸ¤ ìŒì„± ê°ì • ë¶„ì„:")
            for i, (emotion, prob) in enumerate(zip(EMOTIONS, voice_probs)):
                print(f"  {emotion}: {float(prob):.3f}")
        
        if text_probs is not None:
            print("ğŸ“ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„:")
            for i, (emotion, prob) in enumerate(zip(EMOTIONS, text_probs)):
                print(f"  {emotion}: {float(prob):.3f}")
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print("ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        
        self.is_running = True
        
        # ìŒì„± ë…¹ìŒ ì‹œì‘
        self.audio_recorder.start_recording()
        
        # ìŒì„± ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        audio_thread = threading.Thread(target=self.process_audio_thread)
        audio_thread.daemon = True
        audio_thread.start()
        
        try:
            frame_count = 0
            while self.is_running:
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                # ì–¼êµ´ ê°ì • ë¶„ì„ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if frame_count % (30 // VIDEO_FPS) == 0:  # 30fps ê¸°ì¤€
                    face_result = self.face_analyzer.analyze_face(frame)
                    if face_result is not None:
                        face_probs, face_coords = face_result
                        timestamp = datetime.now()
                        
                        # Late Fusionì— ì–¼êµ´ ê°ì • ì¶”ê°€
                        self.late_fusion.add_face_emotion(face_probs)
                        
                        self.results['face'].append(face_probs)
                        
                        print(f"\nğŸ‘¤ ì–¼êµ´ ê°ì • ë¶„ì„ ({timestamp.strftime('%H:%M:%S')}):")
                        for emotion, prob in zip(EMOTIONS, face_probs):
                            print(f"  {emotion}: {float(prob):.3f}")
                
                # Late Fusion: 5ì´ˆë§ˆë‹¤ í†µí•© ê²°ê³¼ ì¶œë ¥
                if self.late_fusion.should_fuse():
                    fusion_result = self.late_fusion.fuse_emotions()
                    if fusion_result is not None:
                        emotion, all_probs, modalities = fusion_result
                        self.late_fusion.print_fusion_result(emotion, all_probs, modalities)
                    self.late_fusion.reset_buffers()
                
                # ì›¹ìº  í™”ë©´ì— ê°ì • ì •ë³´ í‘œì‹œ
                self.display_frame_with_emotions(frame)
                
                # í™”ë©´ ì¶œë ¥
                cv2.imshow('Multimodal Emotion Analysis', frame)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                    
                frame_count += 1
                
        finally:
            self.cleanup()
    
    def display_frame_with_emotions(self, frame):
        """í”„ë ˆì„ì— ê°ì • ì •ë³´ í‘œì‹œ"""
        # ìµœê·¼ ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í™”ë©´ì— í‘œì‹œ
        if self.results['face']:
            face_probs = self.results['face'][-1]
            max_idx = np.argmax(face_probs)
            emotion = EMOTIONS[max_idx]
            confidence = float(face_probs[max_idx])
            
            # í…ìŠ¤íŠ¸ í‘œì‹œ
            text = f"Face: {emotion} ({confidence:.2f})"
            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        if self.results['voice']:
            voice_probs = self.results['voice'][-1]
            if voice_probs is not None:
                max_idx = np.argmax(voice_probs)
                emotion = EMOTIONS[max_idx]
                confidence = float(voice_probs[max_idx])
                
                text = f"Voice: {emotion} ({confidence:.2f})"
                cv2.putText(frame, text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        if self.results['text']:
            text_probs = self.results['text'][-1]
            if text_probs is not None:
                max_idx = np.argmax(text_probs)
                emotion = EMOTIONS[max_idx]
                confidence = float(text_probs[max_idx])
                
                text = f"Text: {emotion} ({confidence:.2f})"
                cv2.putText(frame, text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.is_running = False
        self.audio_recorder.stop_recording()
        self.cap.release()
        cv2.destroyAllWindows()
        
        print("\ní”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.print_summary()
    
    def print_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        print(f"ì´ ì–¼êµ´ ê°ì • ë¶„ì„ íšŸìˆ˜: {len(self.results['face'])}")
        print(f"ì´ ìŒì„± ê°ì • ë¶„ì„ íšŸìˆ˜: {len([x for x in self.results['voice'] if x is not None])}")
        print(f"ì´ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ íšŸìˆ˜: {len([x for x in self.results['text'] if x is not None])}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        analyzer = MultimodalEmotionAnalyzer()
        analyzer.run()
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
