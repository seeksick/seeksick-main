#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ë©”ì¸ í”„ë¡œê·¸ë¨

ì´ í”„ë¡œê·¸ë¨ì€ ë‹¤ìŒ 3ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤:
1. ì–¼êµ´ ê°ì • (seeksick-resnet18.pth)
2. ìŒì„± ê°ì • (seeksick-voice.pt)
3. í…ìŠ¤íŠ¸ ê°ì • (seeksick-kobert.pt)

ëª¨ë“  ê°ì •ì€ [í–‰ë³µ, ìš°ìš¸, ë†€ëŒ, í™”ë‚¨, ì¤‘ë¦½] 5ê°€ì§€ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.
"""

import os
import sys
import time
import threading
import queue
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import numpy as np
import cv2
import torch
import torch.nn.functional as F
import sounddevice as sd
from scipy.io import wavfile
import whisper

# ë¡œì»¬ ëª¨ë¸ ì„í¬íŠ¸
from models import FaceEmotionAnalyzer, VoiceEmotionAnalyzer, TextEmotionAnalyzer, EMOTIONS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì „ì—­ ìƒìˆ˜
SAMPLE_RATE = 16000  # Whisper ê¶Œì¥ ìƒ˜í”Œë ˆì´íŠ¸
AUDIO_BUFFER_SIZE = 3.0  # 3ì´ˆ ë‹¨ìœ„ë¡œ ìŒì„± ë¶„ì„
VIDEO_FPS = 10  # ì–¼êµ´ ê°ì • ë¶„ì„ ì£¼ê¸°

class AudioRecorder:
    """ìŒì„± ë…¹ìŒ ë° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, sample_rate: int = SAMPLE_RATE, buffer_duration: float = AUDIO_BUFFER_SIZE):
        self.sample_rate = sample_rate
        self.buffer_duration = buffer_duration
        self.buffer_size = int(sample_rate * buffer_duration)
        self.audio_queue = queue.Queue()
        self.is_recording = False
        self.audio_data = []
        
    def audio_callback(self, indata, frames, time, status):
        """ìŒì„± ë°ì´í„° ì½œë°± í•¨ìˆ˜"""
        if status:
            logger.warning(f"Audio callback status: {status}")
        self.audio_data.extend(indata[:, 0])  # ëª¨ë…¸ ì±„ë„ë§Œ ì‚¬ìš©
        
        # ë²„í¼ê°€ ê°€ë“ ì°¼ì„ ë•Œ íì— ì¶”ê°€
        if len(self.audio_data) >= self.buffer_size:
            audio_chunk = np.array(self.audio_data[:self.buffer_size], dtype=np.float32)
            self.audio_queue.put(audio_chunk)
            self.audio_data = self.audio_data[self.buffer_size:]
    
    def start_recording(self):
        """ìŒì„± ë…¹ìŒ ì‹œì‘"""
        self.is_recording = True
        self.stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            callback=self.audio_callback,
            blocksize=1024
        )
        self.stream.start()
        logger.info("ìŒì„± ë…¹ìŒì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def stop_recording(self):
        """ìŒì„± ë…¹ìŒ ì¤‘ì§€"""
        self.is_recording = False
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
        logger.info("ìŒì„± ë…¹ìŒì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_audio_chunk(self) -> Optional[np.ndarray]:
        """ìŒì„± ì²­í¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.audio_queue.get_nowait()
        except queue.Empty:
            return None

# ë¶„ì„ê¸° í´ë˜ìŠ¤ë“¤ì€ models íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©

class MultimodalEmotionAnalyzer:
    """ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.audio_recorder = AudioRecorder()
        self.face_analyzer = FaceEmotionAnalyzer()
        self.voice_analyzer = VoiceEmotionAnalyzer()
        self.text_analyzer = TextEmotionAnalyzer()
        
        # Whisper ëª¨ë¸ ë¡œë“œ
        self.whisper_model = whisper.load_model("base")
        logger.info("Whisper ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # ë¹„ë””ì˜¤ ìº¡ì²˜
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            logger.error("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        # ê²°ê³¼ ì €ì¥
        self.results = {
            'face': [],
            'voice': [],
            'text': [],
            'timestamps': []
        }
        
        self.is_running = False
        
    def transcribe_audio(self, audio_data: np.ndarray) -> str:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            # WhisperëŠ” float32 ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
            result = self.whisper_model.transcribe(audio_data)
            text = result["text"].strip()
            logger.info(f"ìŒì„± ì¸ì‹ ê²°ê³¼: {text}")
            return text
        except Exception as e:
            logger.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
            return ""
    
    def process_audio_thread(self):
        """ìŒì„± ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        while self.is_running:
            audio_chunk = self.audio_recorder.get_audio_chunk()
            if audio_chunk is not None:
                timestamp = datetime.now()
                
                # 1. ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = self.transcribe_audio(audio_chunk)
                
                # 2. ìŒì„± ê°ì • ë¶„ì„
                voice_probs = self.voice_analyzer.analyze_voice(audio_chunk)
                
                # 3. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
                text_probs = self.text_analyzer.analyze_text(text) if text else None
                
                # ê²°ê³¼ ì €ì¥
                self.results['voice'].append(voice_probs)
                self.results['text'].append(text_probs)
                self.results['timestamps'].append(timestamp)
                
                # ê²°ê³¼ ì¶œë ¥
                self.print_emotion_results(timestamp, voice_probs, text_probs, text)
            
            time.sleep(0.1)
    
    def print_emotion_results(self, timestamp, voice_probs, text_probs, text=""):
        """ê°ì • ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ì‹œê°„: {timestamp.strftime('%H:%M:%S')}")
        print(f"í…ìŠ¤íŠ¸: {text}")
        print(f"{'='*60}")
        
        if voice_probs is not None:
            print("ğŸ¤ ìŒì„± ê°ì • ë¶„ì„:")
            for i, (emotion, prob) in enumerate(zip(EMOTIONS, voice_probs)):
                print(f"  {emotion}: {prob:.3f}")
        
        if text_probs is not None:
            print("ğŸ“ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„:")
            for i, (emotion, prob) in enumerate(zip(EMOTIONS, text_probs)):
                print(f"  {emotion}: {prob:.3f}")
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print("ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        
        self.is_running = True
        
        # ìŒì„± ë…¹ìŒ ì‹œì‘
        self.audio_recorder.start_recording()
        
        # ìŒì„± ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        audio_thread = threading.Thread(target=self.process_audio_thread)
        audio_thread.daemon = True
        audio_thread.start()
        
        try:
            frame_count = 0
            while self.is_running:
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                # ì–¼êµ´ ê°ì • ë¶„ì„ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if frame_count % (30 // VIDEO_FPS) == 0:  # 30fps ê¸°ì¤€
                    face_probs = self.face_analyzer.analyze_face(frame)
                    if face_probs is not None:
                        timestamp = datetime.now()
                        self.results['face'].append(face_probs)
                        
                        print(f"\nğŸ‘¤ ì–¼êµ´ ê°ì • ë¶„ì„ ({timestamp.strftime('%H:%M:%S')}):")
                        for emotion, prob in zip(EMOTIONS, face_probs):
                            print(f"  {emotion}: {prob:.3f}")
                
                # ì›¹ìº  í™”ë©´ì— ê°ì • ì •ë³´ í‘œì‹œ
                self.display_frame_with_emotions(frame)
                
                # í™”ë©´ ì¶œë ¥
                cv2.imshow('Multimodal Emotion Analysis', frame)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                    
                frame_count += 1
                
        finally:
            self.cleanup()
    
    def display_frame_with_emotions(self, frame):
        """í”„ë ˆì„ì— ê°ì • ì •ë³´ í‘œì‹œ"""
        # ìµœê·¼ ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í™”ë©´ì— í‘œì‹œ
        if self.results['face']:
            face_probs = self.results['face'][-1]
            max_idx = np.argmax(face_probs)
            emotion = EMOTIONS[max_idx]
            confidence = face_probs[max_idx]
            
            # í…ìŠ¤íŠ¸ í‘œì‹œ
            text = f"Face: {emotion} ({confidence:.2f})"
            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        if self.results['voice']:
            voice_probs = self.results['voice'][-1]
            if voice_probs is not None:
                max_idx = np.argmax(voice_probs)
                emotion = EMOTIONS[max_idx]
                confidence = voice_probs[max_idx]
                
                text = f"Voice: {emotion} ({confidence:.2f})"
                cv2.putText(frame, text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        if self.results['text']:
            text_probs = self.results['text'][-1]
            if text_probs is not None:
                max_idx = np.argmax(text_probs)
                emotion = EMOTIONS[max_idx]
                confidence = text_probs[max_idx]
                
                text = f"Text: {emotion} ({confidence:.2f})"
                cv2.putText(frame, text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.is_running = False
        self.audio_recorder.stop_recording()
        self.cap.release()
        cv2.destroyAllWindows()
        
        print("\ní”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.print_summary()
    
    def print_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        print(f"ì´ ì–¼êµ´ ê°ì • ë¶„ì„ íšŸìˆ˜: {len(self.results['face'])}")
        print(f"ì´ ìŒì„± ê°ì • ë¶„ì„ íšŸìˆ˜: {len([x for x in self.results['voice'] if x is not None])}")
        print(f"ì´ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ íšŸìˆ˜: {len([x for x in self.results['text'] if x is not None])}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        analyzer = MultimodalEmotionAnalyzer()
        analyzer.run()
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
